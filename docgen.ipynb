{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bibtexparser\n",
      "  Downloading bibtexparser-1.4.1.tar.gz (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m549.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.3 in /Users/isaac/miniforge3/envs/pytorch/lib/python3.10/site-packages (from bibtexparser) (3.0.9)\n",
      "Building wheels for collected packages: bibtexparser\n",
      "  Building wheel for bibtexparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bibtexparser: filename=bibtexparser-1.4.1-py3-none-any.whl size=43255 sha256=7af7b3c71532fd1d6690cd94f16f286a347511b1865bfb04cdc828147708890b\n",
      "  Stored in directory: /Users/isaac/Library/Caches/pip/wheels/08/c6/c3/56e639fab68d1fdbf13ea147636d9795ccdbd3c1d3178d1332\n",
      "Successfully built bibtexparser\n",
      "Installing collected packages: bibtexparser\n",
      "Successfully installed bibtexparser-1.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bibtexparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import bibtexparser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@article{10.48550/arxiv.1411.1792, \\nyear = {2014}, \\ntitle = {{How transferable are features in deep neural networks?}}, \\nauthor = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.1411.1792}, \\neprint = {1411.1792}, \\nabstract = {{Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Yosinski-How%20transferable%20are%20features%20in%20deep%20neural%20networks--2014-arXiv_1.pdf}\\n}\\n@article{10.48550/arxiv.2106.09685, \\nyear = {2021}, \\ntitle = {{LoRA: Low-Rank Adaptation of Large Language Models}}, \\nauthor = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.2106.09685}, \\neprint = {2106.09685}, \\nabstract = {{An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Hu-LoRA-%20Low-Rank%20Adaptation%20of%20Large%20Language%20Models-2021-arXiv_1.pdf}\\n}\\n@article{10.48550/arxiv.1902.00751, \\nyear = {2019}, \\ntitle = {{Parameter-Efficient Transfer Learning for NLP}}, \\nauthor = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and Laroussilhe, Quentin de and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.1902.00751}, \\neprint = {1902.00751}, \\nabstract = {{Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter\\'s effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4\\\\% of the performance of full fine-tuning, adding only 3.6\\\\% parameters per task. By contrast, fine-tuning trains 100\\\\% of the parameters per task.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Houlsby-Parameter-Efficient%20Transfer%20Learning%20for%20NLP-2019-arXiv.pdf}\\n}\\n@article{10.48550/arxiv.2005.11401, \\nyear = {2020}, \\ntitle = {{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}}, \\nauthor = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.2005.11401}, \\neprint = {2005.11401}, \\nabstract = {{Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Lewis-Retrieval-Augmented%20Generation%20for%20Knowledge-Intensive%20NLP%20Tasks-2020-arXiv.pdf}\\n}\\n@article{10.48550/arxiv.2302.04761, \\nyear = {2023}, \\ntitle = {{Toolformer: Language Models Can Teach Themselves to Use Tools}}, \\nauthor = {Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.2302.04761}, \\neprint = {2302.04761}, \\nabstract = {{Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\\\textbackslash\\\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Schick-Toolformer-%20Language%20Models%20Can%20Teach%20Themselves%20to%20Use%20Tools-2023-arXiv.pdf}\\n}\\n@article{10.48550/arxiv.2305.18290, \\nyear = {2023}, \\ntitle = {{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}}, \\nauthor = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.2305.18290}, \\neprint = {2305.18290}, \\nabstract = {{While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF\\'s ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Rafailov-Direct%20Preference%20Optimization-%20Your%20Language%20Model%20is%20Secretly%20a%20Reward%20Model-2023-arXiv_1.pdf}\\n}\\n@article{10.48550/arxiv.1805.01954, \\nyear = {2018}, \\ntitle = {{Behavioral Cloning from Observation}}, \\nauthor = {Torabi, Faraz and Warnell, Garrett and Stone, Peter}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.1805.01954}, \\neprint = {1805.01954}, \\nabstract = {{Humans often learn how to perform tasks via imitation: they observe others perform a task, and then very quickly infer the appropriate actions to take based on their observations. While extending this paradigm to autonomous agents is a well-studied problem in general, there are two particular aspects that have largely been overlooked: (1) that the learning is done from observation only (i.e., without explicit action information), and (2) that the learning is typically done very quickly. In this work, we propose a two-phase, autonomous imitation learning technique called behavioral cloning from observation (BCO), that aims to provide improved performance with respect to both of these aspects. First, we allow the agent to acquire experience in a self-supervised fashion. This experience is used to develop a model which is then utilized to learn a particular task by observing an expert perform that task without the knowledge of the specific actions taken. We experimentally compare BCO to imitation learning methods, including the state-of-the-art, generative adversarial imitation learning (GAIL) technique, and we show comparable task performance in several different simulation domains while exhibiting increased learning speed after expert trajectories become available.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Torabi-Behavioral%20Cloning%20from%20Observation-2018-arXiv_1.pdf}\\n}\\n@article{10.48550/arxiv.1509.06461, \\nyear = {2015}, \\ntitle = {{Deep Reinforcement Learning with Double Q-learning}}, \\nauthor = {Hasselt, Hado van and Guez, Arthur and Silver, David}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.1509.06461}, \\neprint = {1509.06461}, \\nabstract = {{The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Hasselt-Deep%20Reinforcement%20Learning%20with%20Double%20Q-learning-2015-arXiv_1.pdf}\\n}\\n@article{10.48550/arxiv.1509.02971, \\nyear = {2015}, \\ntitle = {{Continuous control with deep reinforcement learning}}, \\nauthor = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.1509.02971}, \\neprint = {1509.02971}, \\nabstract = {{We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Lillicrap-Continuous%20control%20with%20deep%20reinforcement%20learning-2015-arXiv_1.pdf}\\n}\\n@article{10.48550/arxiv.1801.01290, \\nyear = {2018}, \\ntitle = {{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}}, \\nauthor = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.1801.01290}, \\neprint = {1801.01290}, \\nabstract = {{Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Haarnoja-Soft%20Actor-Critic-%20Off-Policy%20Maximum%20Entropy%20Deep%20Reinforcement%20Learning%20with%20a%20Stochastic%20Actor-2018-arXiv_1.pdf}\\n}\\n@article{10.48550/arxiv.2106.01345, \\nyear = {2021}, \\ntitle = {{Decision Transformer: Reinforcement Learning via Sequence Modeling}}, \\nauthor = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.2106.01345}, \\neprint = {2106.01345}, \\nabstract = {{We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Chen-Decision%20Transformer-%20Reinforcement%20Learning%20via%20Sequence%20Modeling-2021-arXiv_1.pdf}\\n}\\n@article{10.48550/arxiv.1609.03499, \\nyear = {2016}, \\ntitle = {{WaveNet: A Generative Model for Raw Audio}}, \\nauthor = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.1609.03499}, \\neprint = {1609.03499}, \\nabstract = {{This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Oord-WaveNet-%20A%20Generative%20Model%20for%20Raw%20Audio-2016-arXiv_1.pdf}\\n}\\n@article{10.48550/arxiv.2005.08100, \\nyear = {2020}, \\ntitle = {{Conformer: Convolution-augmented Transformer for Speech Recognition}}, \\nauthor = {Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and Pang, Ruoming}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.2005.08100}, \\neprint = {2005.08100}, \\nabstract = {{Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1\\\\%/4.3\\\\% without using a language model and 1.9\\\\%/3.9\\\\% with an external language model on test/testother. We also observe competitive performance of 2.7\\\\%/6.3\\\\% with a small model of only 10M parameters.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Gulati-Conformer-%20Convolution-augmented%20Transformer%20for%20Speech%20Recognition-2020-arXiv_1.pdf}\\n}\\n@article{10.48550/arxiv.2312.00785, \\nyear = {2023}, \\ntitle = {{Sequential Modeling Enables Scalable Learning for Large Vision Models}}, \\nauthor = {Bai, Yutong and Geng, Xinyang and Mangalam, Karttikeya and Bar, Amir and Yuille, Alan and Darrell, Trevor and Malik, Jitendra and Efros, Alexei A}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.2312.00785}, \\neprint = {2312.00785}, \\nabstract = {{We introduce a novel sequential modeling approach which enables learning a Large Vision Model (LVM) without making use of any linguistic data. To do this, we define a common format, \"visual sentences\", in which we can represent raw images and videos as well as annotated data sources such as semantic segmentations and depth reconstructions without needing any meta-knowledge beyond the pixels. Once this wide variety of visual data (comprising 420 billion tokens) is represented as sequences, the model can be trained to minimize a cross-entropy loss for next token prediction. By training across various scales of model architecture and data diversity, we provide empirical evidence that our models scale effectively. Many different vision tasks can be solved by designing suitable visual prompts at test time.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Bai-Sequential%20Modeling%20Enables%20Scalable%20Learning%20for%20Large%20Vision%20Models-2023-arXiv_1.pdf}\\n}\\n@article{10.48550/arxiv.2311.15127, \\nyear = {2023}, \\ntitle = {{Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets}}, \\nauthor = {Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and Jampani, Varun and Rombach, Robin}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.2311.15127}, \\neprint = {2311.15127}, \\nabstract = {{We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for curating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://github.com/Stability-AI/generative-models .}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Blattmann-Stable%20Video%20Diffusion-%20Scaling%20Latent%20Video%20Diffusion%20Models%20to%20Large%20Datasets-2023-arXiv_1.pdf}\\n}\\n@article{10.48550/arxiv.2402.17177, \\nyear = {2024}, \\ntitle = {{Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models}}, \\nauthor = {Liu, Yixin and Zhang, Kai and Li, Yuan and Yan, Zhiling and Gao, Chujie and Chen, Ruoxi and Yuan, Zhengqing and Huang, Yue and Sun, Hanchi and Gao, Jianfeng and He, Lifang and Sun, Lichao}, \\njournal = {arXiv}, \\ndoi = {10.48550/arxiv.2402.17177}, \\neprint = {2402.17177}, \\nabstract = {{Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model\\'s background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora\\'s development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.}}, \\nlocal-url = {file://localhost/Users/isaac/Documents/Papers%20Library/Liu-Sora-%20A%20Review%20on%20Background,%20Technology,%20Limitations,%20and%20Opportunities%20of%20Large%20Vision%20Models-2024-arXiv_1.pdf}\\n}'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../../Downloads/export_2024-3-27.bib\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__builtins__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__closure__',\n",
       " '__code__',\n",
       " '__defaults__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__globals__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__kwdefaults__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__name__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__qualname__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(bibtexparser.bparser.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = bibtexparser.bparser.parse(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {}\n",
    "for i, entry in enumerate(library.entries):\n",
    "    document = {}\n",
    "    document[\"id\"] = str(i)\n",
    "    document[\"addedDate\"] = time.time()\n",
    "    bibtex = entry.copy()\n",
    "    bibtex.pop(\"local-url\")\n",
    "    bibtex.pop(\"ID\")\n",
    "    document[\"bibtex\"] = bibtex\n",
    "    if \"abstract\" in document[\"bibtex\"]:\n",
    "        document[\"bibtex\"][\"abstract\"] = document[\"bibtex\"][\"abstract\"].replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "    if \"title\" in document[\"bibtex\"]:\n",
    "        document[\"bibtex\"][\"title\"] = document[\"bibtex\"][\"title\"].replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "    documents[document[\"id\"]] = document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22701184332322387754\n",
      "90037340089045612381\n",
      "88764297951629466335\n",
      "01905090715475391966\n",
      "97094715314815618253\n",
      "26567952613852015830\n",
      "03362267536865728780\n",
      "19384420171743374029\n",
      "73223695003898760276\n",
      "05534037866803586695\n"
     ]
    }
   ],
   "source": [
    "# Generate random 20 digit id\n",
    "import random\n",
    "\n",
    "\n",
    "def generate_id():\n",
    "    return \"\".join([str(random.randint(0, 9)) for i in range(20)])\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(generate_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = {\n",
    "    \"23625556262883782487\": {\n",
    "        \"id\": \"23625556262883782487\",\n",
    "        \"name\": \"School\",\n",
    "        \"parent\": None,\n",
    "        \"children\": [\"94461546571198391735\", \"10804647432550881374\"],\n",
    "        \"documents\": [\"0\", \"1\"],\n",
    "    },\n",
    "    \"94461546571198391735\": {\n",
    "        \"id\": \"94461546571198391735\",\n",
    "        \"name\": \"Deep Learning\",\n",
    "        \"parent\": \"23625556262883782487\",\n",
    "        \"children\": [\"26694157651817043115\", \"08428438518642337689\"],\n",
    "        \"documents\": [\"2\"],\n",
    "    },\n",
    "    \"26694157651817043115\": {\n",
    "        \"id\": \"26694157651817043115\",\n",
    "        \"name\": \"Papers\",\n",
    "        \"parent\": \"94461546571198391735\",\n",
    "        \"children\": [],\n",
    "        \"documents\": [\"5\", \"6\"],\n",
    "    },\n",
    "    \"08428438518642337689\": {\n",
    "        \"id\": \"08428438518642337689\",\n",
    "        \"name\": \"Websites\",\n",
    "        \"parent\": \"94461546571198391735\",\n",
    "        \"children\": [],\n",
    "        \"documents\": [\"7\", \"8\"],\n",
    "    },\n",
    "    \"10804647432550881374\": {\n",
    "        \"id\": \"10804647432550881374\",\n",
    "        \"name\": \"Machine Perception\",\n",
    "        \"parent\": \"23625556262883782487\",\n",
    "        \"children\": [],\n",
    "        \"documents\": [\"3\", \"4\"],\n",
    "    },\n",
    "    \"06561253512332369082\": {\n",
    "        \"id\": \"06561253512332369082\",\n",
    "        \"name\": \"Work\",\n",
    "        \"parent\": None,\n",
    "        \"children\": [],\n",
    "        \"documents\": [\"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_documents.json\", \"w\") as f:\n",
    "    json.dump({\"documents\": documents, \"folders\": folders}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
